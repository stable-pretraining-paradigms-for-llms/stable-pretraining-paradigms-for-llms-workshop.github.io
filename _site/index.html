<!DOCTYPE html>
<html lang="en-US">

<head>

  
  <meta charset="UTF-8">

  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style"
    type="text/css" crossorigin>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
    integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">

  <!-- Defer loading of KaTeX -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
    integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
    crossorigin="anonymous"></script>

  <!-- KaTeX auto-render extension -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

  <link rel="stylesheet" href="/stable-pretraining-paradigms-for-llms-workshop.github.io/css/cayman.css?v=">

  <!-- Open Graph meta tags (adjust domain and paths accordingly) -->
  <meta property="og:title" content="CAI Workshop 2024">
  <meta property="og:description" content="Stable Pre-Training Paradigms for LLMs">
  <meta property="og:image" content="assets/logo.png">
  <meta property="og:url" content="https://example.com">

  <!-- Favicon (adjust path if needed) -->
  <link rel="icon" href="/stable-pretraining-paradigms-for-llms-workshop.github.io/assets/logo.png" type="image/png">

  <!-- Custom styles -->
  <style>
    body {
      font-family: 'Georgia', serif;
      margin: 2em;
      line-height: 1.6;
      color: #333;
    }

    h1, h2, h3 {
      text-align: center;
    }

    b {
      color: #2c3e50;
    }

    a {
      color: #2980b9;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    ol li {
      margin-bottom: 1em;
    }

    #paper-submission,
    #key-dates,
    #paper-guidelines {
      margin-top: 2em;
    }

    /* tab styles */
    .tab {
      overflow: hidden;
      border: 1px solid #cccccc;
      background-color: #fdffff;
    }

    .tab button {
      background-color: inherit;
      float: left;
      border: none;
      outline: none;
      cursor: pointer;
      padding: 14px 16px;
      transition: 0.3s;
    }

    .tab button:hover {
      background-color: #dddddd;
    }

    .tab button.active {
      background-color: #cccccc;
    }

    .tabcontent {
      display: none;
      padding: 6px 12px;
      border: 1px solid #cccccc;
      border-top: none;
    }
  </style>

</head>

<body>
  <header class="page-header" role="banner">
    <h1 class="project-name">Stable Pre-Training Paradigms for LLMs <br> Reducing Instability, Increasing Capacity</h1>
    <h2 class="project-tagline">CAI Workshop 2025</h2>
    
    <a href="index" class="btn">Workshop</a>
    
    <a href="speakers" class="btn">Speakers</a>
    
    <a href="organizers" class="btn">Organizers</a>
    
    <a href="venue" class="btn">Venue</a>
    
    <a href="schedule" class="btn">Schedule</a>
    
    <a href="question" class="btn">Q&A</a>
    
  </header>

  <main id="content" class="main-content" role="main">
    <style>
body {
  font-family: 'Georgia', serif;
  margin: 2em;
  line-height: 1.6;
  color: #333;
}

h1, h2, h3 {
  text-align: center;
}

b {
  color: #2c3e50;
}

a {
  color: #2980b9;
  text-decoration: none;
}
a:hover {
  text-decoration: underline;
}

ol li {
  margin-bottom: 1em;
}

#paper-submission,
#key-dates,
#paper-guidelines {
  margin-top: 2em;
}
</style>

<p style="text-align: justify; font-size: 1.1em;">
<b>Our workshop aims to develop stable pre-training paradigms for LLMs that ensure consistent performance and reliability in Large Language Models (LLMs).</b>
</p>

<h1>Topic and Content</h1>

<p style="text-align: justify; font-size: 1em;">
Large Language Models (LLMs) have revolutionized artificial intelligence by achieving remarkable performance across a wide array of tasks. However, the pre-training process of these models often encounters instability issues such as loss spikes, gradient vanishing or exploding, and convergence difficulties. These instabilities not only prolong training time but also affect the overall performance and reliability of the models. As LLMs become increasingly integral to various applications, establishing stable training paradigms is essential. This workshop seeks to bring together researchers and practitioners to discuss and develop strategies for enhancing the stability of LLM pre-training. By focusing on aspects like data quality, optimizer selection, architectural innovations, and spike-awareness mechanisms, we aim to foster collaborations that lead to more robust and dependable LLMs.
</p>

<p style="text-align: justify; font-size: 1em;">
We will cover a range of topics that contribute to the stability of LLM pre-training, including but not limited to:
</p>

<ol style="font-size: 1em;">
  <li>
    <b>Data Quality and Preprocessing for Stability</b>
    <p style="text-align: justify;">Investigating how high-quality, well-preprocessed data can enhance training stability. Topics include data cleaning, balancing, augmentation, and the impact of data diversity on preventing overfitting and promoting smooth convergence.</p>
  </li>
  <li>
    <b>Advanced Optimizers for Stable Training</b>
    <p style="text-align: justify;">Exploring optimization algorithms that improve training stability, such as adaptive learning rates, momentum methods, and second-order optimizers. Discussion on how these optimizers can mitigate issues like loss spikes and facilitate consistent gradient flow.</p>
  </li>
  <li>
    <b>Architectural Innovations Promoting Stability</b>
    <p style="text-align: justify;">Examining model architectures that inherently support stable training and prevent vanishing or exploding gradients.</p>
  </li>
  <li>
    <b>Spike-Awareness and Mitigation Techniques</b>
    <p style="text-align: justify;">Developing methods to detect and respond to training instabilities in real-time, focusing specifically on loss spikesâ€”sudden increases during model training. These spikes can signal problems like vanishing or exploding gradients, overfitting, or inappropriate learning rates.</p>
  </li>
  <li>
    <b>Efficient Hardware Utilization for Stability</b>
    <p style="text-align: justify;">Leveraging hardware accelerators and memory management strategies that support stable training of large models. Topics may include gradient checkpointing, mixed-precision training, and specialized hardware to handle extensive computations reliably.</p>
  </li>
  <li>
    <b>Case Studies and Best Practices</b>
    <p style="text-align: justify;">Sharing experiences from successful implementations of stable LLM training, including challenges faced and solutions developed. This includes industry applications where stability was critical for deployment and performance.</p>
  </li>
</ol>

<div id="paper-submission">
<h2>Paper Submission</h2>
<p style="text-align: justify;">Please submit your papers via <a href="https://easychair.org/conferences/?conf=cai2025" target="_blank">EasyChair</a>.</p>
</div>

<div id="key-dates">
<h2>Key Dates</h2>
<ul style="list-style-type: none; font-size: 1em; padding:0;">
  <li><b>Paper Submission Deadline:</b> 15 January, 2025 AoE</li>
  <li><b>Notification of Acceptance:</b> 1 March, 2025</li>
  <li><b>Final Camera-Ready Copy Deadline:</b> 7 March, 2025</li>
  <li><b>Workshop Date:</b> During IEEE CAI 2025</li>
</ul>
</div>

<div id="paper-guidelines">
<h2>Paper Guidelines</h2>
<p style="text-align: justify;">
All submissions must adhere to the following formatting requirements:
</p>
<ul style="font-size: 1em;">
  <li>Submissions must be original and not currently under review elsewhere.</li>
  <li>Use the IEEE style files for conference proceedings, available at <a href="https://template-selector.ieee.org/secure/templateSelector/publicationType" target="_blank">IEEE Templates</a>.</li>
  <li>Follow double-blind reviewing: In LaTeX, use:
    <pre><code>\author{\IEEEauthorblockN{Anonymous Authors}}</code></pre></li>
  <li>Only PDF format is accepted.</li>
  <li>Paper Size: A4 (210mm x 297mm).</li>
  <li>Length:
    <ul>
      <li>Long papers: up to 6 pages (plus up to 2 extra pages with additional charge).</li>
      <li>Abstract papers: up to 2 pages (plus up to 2 extra pages with additional charge).</li>
    </ul>
  </li>
  <li>Formatting: double column, single spaced, 10-point Times Roman font. Use the official IEEE style files.</li>
  <li>No page numbers (they will be inserted later).</li>
  <li>No new authors can be added after the submission deadline.</li>
</ul>
</div>

    <footer class="site-footer">
      <hr>
      <!-- Add footer content here if needed -->
    </footer>
  </main>

</body>

</html>
